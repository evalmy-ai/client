---
title: "Batch evaluation"
format: html
editor: visual
---

In real-world testing, it is practical to define a complete 
test set containing multiple questions and answers. The 
Evaluator class includes several convenient methods to 
handle this for you.

## Simple python lists

The most straighforward way to create a test batch is to concatenate multiple
entries to the list and call the *evaluate_batch* method.

```python
data = [
   {
      "expected": "Sunny.",
      "actual": "Raining.",
   },
   {
      "expected": "Cloudy.",
      "actual": "Storm.",
   }
]

results, errors = ev.evaluate_batch(data, context='Compare precipitation only. Ignore other weather traits.')
```

The *evaluate_batch* method returns a tuple of lists:

- **results**: A list of evaluation results, with None values if an error occurs.
- **errors**: A list of evaluation errors, with None values if no error occurs.

## Pandas dataset

```python
import pandas as pd

data = pd.DataFrame({
    "expected": ["We live in a best of worlds!"] * 3,
    "actual": ["Life is great!", "Life has its ups and downs!", "Life is miserable!"]
}, index=["optimist", "realist", "pessimist"])

print(data)
```

|           | expected                     | actual                      |
|:----------|:-----------------------------|:----------------------------|
| optimist  | We live in a best of worlds! | Life is great!              |
| realist   | We live in a best of worlds! | Life has its ups and downs! |
| pessimist | We live in a best of worlds! | Life is miserable!          |

```python
result = evaluator.evaluate_dataset(data, context="Compare the statements regarding their expected life outcomes.")

print(result)
```

|   | expected  | actual  | context  | score_con  | reason_con                       | error  |
|:--|:----------|:--------|:---------|-----------:|:---------------------------------|:-------|
| optimist  | We live in a best of worlds! | Life is great!              |           |           1.0 | {'statements': [{'reasoning': "...", 'summary': 'Both texts express a positive outlook on life.', 'severity': 'negligible'}]}                                                                                                                                                                                            |         |
| realist   | We live in a best of worlds! | Life has its ups and downs! |           |           0.5 | {'statements': [{'reasoning': "...", 'summary': 'Difference in the portrayal of life', 'severity': 'large'}]} |         |
| pessimist | We live in a best of worlds! | Life is miserable!          |           |           0.0 | {'statements': [{'reasoning': "...", 'summary': 'Opposing views on the quality of life', 'severity': 'critical'}]}  |   |

The *evaluate_dataset* method returns a Pandas DataFrame 
indexed by the same values as the input, with the following 
columns:

- **expected**: original expected values
- **actual**: original actual values
- **context**: context used for evaluation
- **score_con**: contradiction score
- **reason_con**: contradiction reasoning
- **error**: evaluation error or None

## Test case defined in json

It might be convenient to define the entire test scenario using a JSON file.

```json
{
  "name": "A sample rag testcase",

  "context": "Evaluate the answers to the school test. Be careful to factual clarity and particular numbers. Formulation and style is not relevant.",

  "scoring": {
    "contradictions": {
      "name": "linear",
      "params": {
        "weights": {
          "critical": 1,
          "large": 0.5,
          "small": 0.1,
          "negligible": 0
        }
      }
    }
  },

  "items": [
    {
      "context": "Question: What is the capital of France?",
      "expected": "The capital of France is Paris",
      "actual": "Paris."
    },
    {
      "context": "Question: How long is Great Wall of China?",
      "expected": "GWCh is more than 21 thousand kilometers long.",
      "actual": "It is 13 thousand miles."
    },
    {
      "context": "Question: How long was hundred years war?",
      "expected": "116 years, 4 months, 3 weeks and 4 days.",
      "actual": "A hundred years."
    }
  ]
}
```

The scenario is evaluated by calling the *evaluate_test_case* method.

```python
with open('test_case_input.json') as fi:
    test_case = json.load(fi)

result = evaluator.evaluate_test_case(test_case=test_case)

with open('test_case_output.json', 'w') as fo:
    json.dump(result, fo, ensure_ascii=False, indent=4)
```

It is also possible to input the actual values directly into 
the *evaluate_test_case* method, rather than including them 
in the test scenario definition.

```python
values = ["Paris.", "It is 13 thousand miles.", "A hundred years."]
result = evaluator.evaluate_test_case(test_case=test_case, actual_values=values)
print(result)
```

```json
{
    "items": [
        {
            "expected": "The capital of France is Paris",
            "actual": "Paris.",
            "context": "Evaluate the answers to the school test. Be careful to factual clarity and particular numbers. Formulation and style is not relevant.\nQuestion: What is the capital of France?",
            "contradictions": {
                "score": 1.0,
                "reasoning": {
                    "statements": [
                        {
                            "reasoning": "Both <TEXT 1> and <TEXT 2> provide the correct answer to the question about the capital of France, which is Paris. There is no contradiction between the two texts as they both state the same fact, despite <TEXT 2> being more concise. The criterion for evaluation is factual clarity and particular numbers, and both texts meet this criterion with respect to the capital of France.",
                            "summary": "Both texts identify the capital of France correctly.",
                            "severity": "negligible"
                        }
                    ]
                }
            }
        },
        {
            "expected": "GWCh is more than 21 thousand kilometers long.",
            "actual": "It is 13 thousand miles.",
            "context": "Evaluate the answers to the school test. Be careful to factual clarity and particular numbers. Formulation and style is not relevant.\nQuestion: How long is Great Wall of China?",
            "contradictions": {
                "score": 0.9,
                "reasoning": {
                    "statements": [
                        {
                            "reasoning": "The statement from <TEXT 1> claims that the Great Wall of China (GWCh) is more than 21 thousand kilometers long. In <TEXT 2>, the length is given as 13 thousand miles. To determine if there is a contradiction, we need to convert miles to kilometers or vice versa. Since 1 mile is approximately 1.60934 kilometers, 13 thousand miles would be approximately 20,921.42 kilometers. The two lengths given are relatively close, with <TEXT 1> suggesting a slightly longer length than what the conversion of <TEXT 2> would suggest. However, the Great Wall of China is commonly cited as being approximately 21,196 kilometers long, which aligns more closely with <TEXT 1>. Given the context of evaluating a school test where factual clarity and particular numbers are important, this discrepancy in length is significant enough to be considered a contradiction. However, since both texts are attempting to convey that the Great Wall is very long and the numbers are not drastically different, the severity of this contradiction is small.",
                            "summary": "Different lengths of the Great Wall of China are provided.",
                            "severity": "small"
                        }
                    ]
                }
            }
        },
        {
            "expected": "116 years, 4 months, 3 weeks and 4 days.",
            "actual": "A hundred years.",
            "context": "Evaluate the answers to the school test. Be careful to factual clarity and particular numbers. Formulation and style is not relevant.\nQuestion: How long was hundred years war?",
            "contradictions": {
                "score": 0.5,
                "reasoning": {
                    "statements": [
                        {
                            "reasoning": "According to <TEXT 1>, the Hundred Years' War lasted '116 years, 4 months, 3 weeks and 4 days,' which is a precise duration that exceeds a hundred years. In contrast, <TEXT 2> simplifies the duration to 'A hundred years,' which is factually incorrect when considering the specific length provided in <TEXT 1>. Given the context of evaluating school test answers where factual accuracy, especially regarding numbers, is crucial, this constitutes a large contradiction because it is a significant error in a factual detail that affects the correctness of the answer.",
                            "summary": "Difference in the duration of the Hundred Years' War",
                            "severity": "large"
                        }
                    ]
                }
            }
        }
    ]
}
```



