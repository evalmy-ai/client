---
title: "Batch evaluation"
format: html
editor: visual
---

In real-world testing, it is practical to define a complete test set containing multiple questions and answers. The Evaluator class includes several convenient methods to handle this for you.

## Simple python lists

The most straightforward way to create a test batch is to concatenate multiple entries to the list and call the *evaluate_batch* method.

``` python
data = [
   {
      "expected": "Sunny.",
      "actual": "Raining.",
   },
   {
      "expected": "Cloudy.",
      "actual": "Storm.",
   }
]

results, errors = evaluator.evaluate_batch(data)
```

The *evaluate_batch* method returns a tuple of lists:

-   **results**: A list of evaluation results, with None values if an error occurs.
-   **errors**: A list of evaluation errors, with None values if no error occurs.

## Pandas dataset

``` python
import pandas as pd

data = pd.DataFrame({
    "expected": ["We live in a best of worlds!"] * 3,
    "actual": ["Life is great!", "Life has its ups and downs!", "Life is miserable!"]
}, index=["optimist", "realist", "pessimist"])

print(data)
```

|           | expected                     | actual                      |
|:----------|:-----------------------------|:----------------------------|
| optimist  | We live in a best of worlds! | Life is great!              |
| realist   | We live in a best of worlds! | Life has its ups and downs! |
| pessimist | We live in a best of worlds! | Life is miserable!          |

``` python
result = evaluator.evaluate_dataset(data, context="Compare the statements regarding their expected life outcomes.")

print(result)
```

|           | expected                     | actual                      | context | score_con | reason_con                                                                                                                      | error |
|:----------|:----------|:----------|:----------|----------:|:----------|:----------|
| optimist  | We live in a best of worlds! | Life is great!              |         |       1.0 | {'statements': \[{'reasoning': "...", 'summary': 'Both texts express a positive outlook on life.', 'severity': 'negligible'}\]} |       |
| realist   | We live in a best of worlds! | Life has its ups and downs! |         |       0.5 | {'statements': \[{'reasoning': "...", 'summary': 'Difference in the portrayal of life', 'severity': 'large'}\]}                 |       |
| pessimist | We live in a best of worlds! | Life is miserable!          |         |       0.0 | {'statements': \[{'reasoning': "...", 'summary': 'Opposing views on the quality of life', 'severity': 'critical'}\]}            |       |

The *evaluate_dataset* method returns a Pandas DataFrame indexed by the same values as the input, with the following columns:

-   **expected**: original expected values
-   **actual**: original actual values
-   **context**: context used for evaluation
-   **score_con**: contradiction score
-   **reason_con**: contradiction reasoning
-   **error**: evaluation error or None

## Test case defined in JSON

It might be convenient to define the entire test scenario using a JSON file.

``` json
{
  "name": "A sample rag testcase",

  "context": "Evaluate the answers to the school test. Be careful to factual clarity and particular numbers. Formulation and style is not relevant.",

  "scoring": {
    "contradictions": {
      "name": "linear",
      "params": {
        "weights": {
          "critical": 1,
          "large": 0.5,
          "small": 0.1,
          "negligible": 0
        }
      }
    }
  },

  "items": [
    {
      "context": "Question: What is the capital of France?",
      "expected": "The capital of France is Paris",
      "actual": "Paris."
    },
    {
      "context": "Question: How long is Great Wall of China?",
      "expected": "GWCh is more than 21 thousand kilometers long.",
      "actual": "It is 13 thousand miles."
    },
    {
      "context": "Question: How long was hundred years war?",
      "expected": "116 years, 4 months, 3 weeks and 4 days.",
      "actual": "A hundred years."
    }
  ]
}
```

The scenario is evaluated by calling the *evaluate_test_case* method.

``` python
with open('test_case_input.json') as fi:
    test_case = json.load(fi)

result = evaluator.evaluate_test_case(test_case=test_case)

with open('test_case_output.json', 'w') as fo:
    json.dump(result, fo, ensure_ascii=False, indent=4)
```

It is also possible to input the actual values directly into the *evaluate_test_case* method, rather than including them in the test scenario definition.

``` python
values = ["Paris.", "It is 13 thousand miles.", "A hundred years."]
result = evaluator.evaluate_test_case(test_case=test_case, actual_values=values)
print(result)
```

``` json
{
    "name": "A sample rag testcase",
    "context": "Evaluate the answers to the school test. Be careful to factual clarity and particular numbers. Formulation and style is not relevant.",
    "scoring": {
        "contradictions": {
            "name": "linear",
            "params": {
                "weights": {
                    "critical": 1,
                    "large": 0.5,
                    "small": 0.1,
                    "negligible": 0
                }
            }
        }
    },
    "items": [
        {
            "context": "Evaluate the answers to the school test. Be careful to factual clarity and particular numbers. Formulation and style is not relevant.\nQuestion: What is the capital of France?",
            "expected": "The capital of France is Paris",
            "actual": "Paris.",
            "contradictions": {
                "scores": {
                    "score": 1.0
                },
                "reasoning": {
                    "statements": []
                }
            }
        },
        {
            "context": "Evaluate the answers to the school test. Be careful to factual clarity and particular numbers. Formulation and style is not relevant.\nQuestion: How long is Great Wall of China?",
            "expected": "GWCh is more than 21 thousand kilometers long.",
            "actual": "It is 13 thousand miles.",
            "contradictions": {
                "scores": {
                    "score": 0.9
                },
                "reasoning": {
                    "statements": [
                        {
                            "severity": "small",
                            "summary": "Different lengths of the Great Wall of China are provided.",
                            "reasoning": "The statement from <TEXT 1> claims that the Great Wall of China (GWCh) is more than 21 thousand kilometers long. In contrast, <TEXT 2> states that it is 13 thousand miles long. To determine if there is a contradiction, we need to convert miles to kilometers. 13 thousand miles is approximately 20,921 kilometers, which is slightly less than the length stated in <TEXT 1>. Given the context of evaluating a school test where factual clarity and particular numbers are important, this discrepancy in length is a small contradiction because it could lead to a different assessment of the student's answer."
                        }
                    ]
                }
            }
        },
        {
            "context": "Evaluate the answers to the school test. Be careful to factual clarity and particular numbers. Formulation and style is not relevant.\nQuestion: How long was hundred years war?",
            "expected": "116 years, 4 months, 3 weeks and 4 days.",
            "actual": "A hundred years.",
            "contradictions": {
                "scores": {
                    "score": 0.5
                },
                "reasoning": {
                    "statements": [
                        {
                            "severity": "large",
                            "summary": "Difference in the duration of the Hundred Years' War.",
                            "reasoning": "The statement from <TEXT 1> provides a precise duration of the Hundred Years' War as '116 years, 4 months, 3 weeks and 4 days,' which is factually accurate. In contrast, <TEXT 2> simplifies the duration to 'A hundred years,' which is factually incorrect and misleading given the context of a school test where factual clarity and particular numbers are important. This is a large contradiction because it is a significant error in a factual detail that impacts the understanding of the historical duration of the war."
                        }
                    ]
                }
            }
        }
    ]
}
```